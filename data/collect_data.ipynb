{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "import multiprocess\n",
    "import json\n",
    "import glob\n",
    "import scipy.stats\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common import Braid, phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_braids = pickle.load(open(\"raw_data/mpo_data_d0.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_braids2 = pickle.load(open(\"raw_data/braids_n10_29_l5_50_c50_d8_1024.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "braids = [\n",
    "    {**a, **b} for a, b in zip(orig_braids, orig_braids2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_depth(strands, clist):\n",
    "    depths = [0] * strands\n",
    "    for idx, _ in clist:\n",
    "        depths[idx] = depths[idx + 1] = max(depths[idx], depths[idx + 1]) + 1\n",
    "    return max(depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossings = np.array([len(b['crossing_list']) for b in braids])\n",
    "qubits = np.array([b['strands'] + 1 for b in braids])\n",
    "\n",
    "abs_errs = np.array([\n",
    "    [abs(b['mpo-0']['estimate'] - b[k]) for b in braids] for k in ['mpo-8', 'mpo-16', 'mpo-32', 'mpo-64', 'mpo-128', 'mpo-256', 'mpo-512', 'mpo-1024']\n",
    "])\n",
    "\n",
    "rel_errs = np.array([\n",
    "    [abs(b['mpo-0']['estimate'] - b[k])/abs(b['mpo-0']['estimate']) for b in braids] for k in ['mpo-8', 'mpo-16', 'mpo-32', 'mpo-64', 'mpo-128', 'mpo-256', 'mpo-512', 'mpo-1024']\n",
    "])\n",
    "\n",
    "max_chi = np.array([max(max(s) for s in b['mpo-0']['bond_sizes']) for b in braids])\n",
    "\n",
    "max_intermediate = np.array([\n",
    "    max(\n",
    "        4 * (cs[0] + cs[-1] + sum(cs[i] * cs[i + 1] for i in range(len(cs) - 1)))\n",
    "        for cs in b['mpo-0']['bond_sizes']\n",
    "    )\n",
    "    for b in braids\n",
    "])\n",
    "\n",
    "depths = np.array([calculate_depth(r['strands'], r['crossing_list']) for r in braids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_rel_errs = np.concatenate([\n",
    "    np.stack([8*2**i / max_chi, rel_errs[i, :]], axis=1)\n",
    "    for i in range(8)\n",
    "], axis=0)\n",
    "\n",
    "normalized_rel_errs_capped = normalized_rel_errs[normalized_rel_errs[:, 0] <= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:23<00:00, 420.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_max_intermediate_for_chi(idx):\n",
    "    b = braids[idx]\n",
    "    mchi = max_chi[idx]\n",
    "    return [max(\n",
    "        4 * (min(cs[0], mchi*f) + min(cs[-1], mchi*f) + sum(min(cs[i], mchi*f) * min(cs[i + 1], mchi*f) for i in range(len(cs) - 1)))\n",
    "        for cs in b['mpo-0']['bond_sizes']\n",
    "    ) for f in np.linspace(0.0, 1.0, 20)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = multiprocess.Pool(64) \n",
    "    max_intermediate_chi = np.array(list(tqdm.tqdm(pool.imap(compute_max_intermediate_for_chi, range(len(braids))), total=len(braids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1938.70it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_max_intermediate_for_chi2(idx):\n",
    "    b = braids[idx]\n",
    "    return [max(\n",
    "        4 * (min(cs[0], f) + min(cs[-1], f) + sum(min(cs[i], f) * min(cs[i + 1], f) for i in range(len(cs) - 1)))\n",
    "        for cs in b['mpo-0']['bond_sizes']\n",
    "    ) for f in [8, 16, 32, 64, 128, 256, 512, 1024]]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = multiprocess.Pool(64) \n",
    "    max_intermediate_chi2 = np.array(list(tqdm.tqdm(pool.imap(compute_max_intermediate_for_chi2, range(len(braids))), total=len(braids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_rel_errsa = np.concatenate([\n",
    "    np.stack([max_intermediate_chi2[:, i] / max_intermediate, rel_errs[i, :]], axis=1)\n",
    "    for i in range(8)\n",
    "], axis=0)\n",
    "\n",
    "normalized_rel_errsa_capped = normalized_rel_errsa[normalized_rel_errsa[:, 0] <= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_cost(dims, idx):\n",
    "    chi0 = 1 if idx == 0 else dims[idx - 1]\n",
    "    chi1 = dims[idx]\n",
    "    chi2 = 1 if idx == len(dims) - 1 else dims[idx + 1]\n",
    "    return chi1**2 * (8*chi0 + 8*chi2 - 2*chi1 / 3)\n",
    "\n",
    "def multiply_cost(dims, idx, k=3):\n",
    "    if k == 3:\n",
    "        chi0 = 1 if idx == 0 else dims[idx - 1]\n",
    "        chi1 = dims[idx]\n",
    "        chi2 = dims[idx + 1]\n",
    "        chi3 = 1 if idx == len(dims) - 2 else dims[idx + 2]\n",
    "        return 32 * (chi0*chi1 + 2 * chi1*chi2 + chi2*chi3)\n",
    "    elif k == 2:\n",
    "        chi0 = 1 if idx == 0 else dims[idx - 1]\n",
    "        chi1 = dims[idx]\n",
    "        chi2 = 1 if idx == len(dims) - 1 else dims[idx + 1]\n",
    "        return 32 * (chi0*chi1 + chi1*chi2)\n",
    "\n",
    "def compress_cost(dims, dims_after, idx, dir=\"left\"):\n",
    "    chi0 = 1 if idx == 0 else dims[idx - 1]\n",
    "    chi1 = dims[idx]\n",
    "    chi1p = dims_after[idx]\n",
    "    chi2 = 1 if idx == len(dims) - 1 else dims[idx + 1]\n",
    "    if dir == \"right\":\n",
    "        chi0, chi2 = chi2, chi0\n",
    "    return 8*chi0*chi1*chi1p + (80/3) * chi1 * chi2 * min(chi1, 4*chi2)\n",
    "\n",
    "def compute_flops_cost(braid, printd=False, chi=float('inf')):\n",
    "    total = 0\n",
    "    dirs = [None] * braid['strands']\n",
    "    for (idx, _), dims, dims_after in zip(braid['crossing_list'][1:], braid['mpo-0']['bond_sizes'], braid['mpo-0']['bond_sizes'][1:]):\n",
    "        dims = [min(d, chi) for d in dims.copy()]\n",
    "        dims_after = [min(d, chi) for d in dims_after.copy()]\n",
    "\n",
    "        if printd:\n",
    "            print(\"A:\", '*'+'*'.join('>' if d == \"right\" else '<' if d == 'left' else '-' for d in dirs)+'*')\n",
    "\n",
    "        # Canonicalize around idx\n",
    "        for j in range(idx):\n",
    "            if dirs[j] != \"right\":\n",
    "                total += canonicalize_cost(dims, idx)\n",
    "                dirs[j] = \"right\"\n",
    "        for j in range(len(dirs) - 1, idx + 1, -1):\n",
    "            if dirs[j] != \"left\":\n",
    "                total += canonicalize_cost(dims, idx)\n",
    "                dirs[j] = \"left\"\n",
    "\n",
    "        if printd:\n",
    "            print(\"B:\", '*'+'*'.join('>' if d == \"right\" else '<' if d == 'left' else '-' for d in dirs)+'*')\n",
    "            \n",
    "        # Multiply by U\n",
    "        total += multiply_cost(dims, idx)\n",
    "        dirs[idx] = None\n",
    "        dirs[idx + 1] = None\n",
    "        dims[idx] *= 2\n",
    "        dims[idx + 1] *= 2\n",
    "\n",
    "        if printd:\n",
    "            print(\"C:\", '*'+'*'.join('>' if d == \"right\" else '<' if d == 'left' else '-' for d in dirs)+'*')\n",
    "\n",
    "        # Move center to idx\n",
    "        total += canonicalize_cost(dims, idx + 1)\n",
    "        dirs[idx + 1] = \"left\"\n",
    "        total += canonicalize_cost(dims, idx)\n",
    "        dirs[idx] = \"left\"\n",
    "\n",
    "        if printd:\n",
    "            print(\"D:\", '*'+'*'.join('>' if d == \"right\" else '<' if d == 'left' else '-' for d in dirs)+'*')\n",
    "            \n",
    "        # Compress bonds rightwards\n",
    "        total += compress_cost(dims, dims_after, idx, dir=\"right\")\n",
    "        total += compress_cost(dims, dims_after, idx + 1, dir=\"right\")\n",
    "        dirs[idx] = \"right\"\n",
    "        dirs[idx + 1] = \"right\"\n",
    "\n",
    "        if printd:\n",
    "            print(\"E:\", '*'+'*'.join('>' if d == \"right\" else '<' if d == 'left' else '-' for d in dirs)+'*')\n",
    "\n",
    "    # Single sweep to set up for projectors\n",
    "    for j in range(len(dirs) - 1, -1, -1):\n",
    "        if dirs[j] != \"left\":\n",
    "            total += canonicalize_cost(dims, idx)\n",
    "            dirs[j] = \"left\"\n",
    "\n",
    "    for idx, dims in enumerate(braid['mpo-0']['bond_sizes'][len(braid['crossing_list']) - 1:-1]):\n",
    "        dims_after = [min(d, chi) for d in dims_after.copy()]\n",
    "        dims = [min(d, chi) for d in dims.copy()]\n",
    "        if printd:\n",
    "            print(\"F:\", '*'+'*'.join('>' if d == \"right\" else '<' if d == 'left' else '-' for d in dirs)+'*')\n",
    "            \n",
    "        # Multiply by proj:\n",
    "        total += multiply_cost(dims, idx, k=2)\n",
    "        dirs[idx] = None\n",
    "        dims[idx] *= 2\n",
    "\n",
    "        if printd:\n",
    "            print(\"G:\", '*'+'*'.join('>' if d == \"right\" else '<' if d == 'left' else '-' for d in dirs)+'*')\n",
    "            \n",
    "        # Compress bond rightwards\n",
    "        total += compress_cost(dims, dims_after, idx, dir=\"right\")\n",
    "        dirs[idx] = \"right\"\n",
    "\n",
    "        if printd:\n",
    "            print(\"H:\", '*'+'*'.join('>' if d == \"right\" else '<' if d == 'left' else '-' for d in dirs)+'*')\n",
    "\n",
    "    # Trace\n",
    "    dims = braid['mpo-0']['bond_sizes'][-1]\n",
    "    total += 4 * (dims[0] + dims[-1])\n",
    "    total += 4 * sum(a * b for a, b in zip(dims, dims[1:]))\n",
    "\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:16<00:00, 613.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def flops_for_braid(idx):\n",
    "    braid = braids[idx]\n",
    "    mchi = max_chi[idx]\n",
    "    return [compute_flops_cost(braid, chi=factor*mchi) for factor in np.linspace(0, 1, 20)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pool = multiprocess.Pool(64) \n",
    "    flops = np.array(list(tqdm.tqdm(pool.imap(flops_for_braid, range(len(braids))), total=len(braids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_flops = np.concatenate([\n",
    "    np.stack([max_intermediate_chi[:, i] / max_intermediate, flops[:, i] / flops[:, -1]], axis=1)\n",
    "    for i in range(20)\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_flops_chi = np.concatenate([\n",
    "    np.stack([np.repeat(j, len(flops[:, -1])), flops[:, i] / flops[:, -1]], axis=1)\n",
    "    for i, j in enumerate(np.linspace(0, 1, 20))\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"processed/mpo_props.npz\", \n",
    "    qubits=qubits, crossings=crossings, depths=depths, abs_errs=abs_errs, rel_errs=rel_errs,\n",
    "    max_chi=max_chi, max_intermediate=max_intermediate, flops=flops, max_intermediate_chi=max_intermediate_chi, max_intermediate_chi2=max_intermediate_chi2\n",
    ")\n",
    "\n",
    "np.savez(\"processed/mpo_aggregate.npz\",\n",
    "    normalized_flops_im=normalized_flops, normalized_flops_chi=normalized_flops_chi,\n",
    "    normalized_rel_errs_chi=normalized_rel_errs_capped, normalized_rel_errs_im=normalized_rel_errsa_capped,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Simulation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = np.load(\"raw_data/results_small_cross.npz\", allow_pickle=True)\n",
    "data_1 = np.load(\"raw_data/results_small_1e-4_cross.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23819/23819 [00:04<00:00, 4855.40it/s]\n"
     ]
    }
   ],
   "source": [
    "crossings = np.array([len(c) for c in data_5['crossings']])\n",
    "qubits = data_5['qubits']\n",
    "\n",
    "abs_errs = np.abs(data_5['acs'] - data_5['ms'])\n",
    "abs_errsn = np.abs(data_5['acs'] - data_5['msn'])\n",
    "rel_errs = np.abs(data_5['acs'] - data_5['ms']) / np.abs(data_5['acs'])\n",
    "rel_errsn = np.abs(data_5['acs'] - data_5['msn']) / np.abs(data_5['acs'])\n",
    "\n",
    "dr = data_5['dr']\n",
    "gates = data_5['gates']\n",
    "\n",
    "crossings_data = data_5['crossings']\n",
    "depths = np.array([calculate_depth(qubits[idx] - 1, [(abs(c) - 1, c/abs(c)) for c in crossings_data[idx]]) for idx in tqdm.trange(len(qubits))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"processed/sims_5_props.npz\",\n",
    "    crossings=crossings, qubits=qubits, depths=depths,\n",
    "    abs_errs=abs_errs, abs_errsn=abs_errsn, rel_errs=rel_errs, rel_errsn=rel_errsn,\n",
    "    gates=gates, dr=dr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23819/23819 [00:04<00:00, 4968.79it/s]\n"
     ]
    }
   ],
   "source": [
    "crossings = np.array([len(c) for c in data_1['crossings']])\n",
    "qubits = data_1['qubits']\n",
    "\n",
    "abs_errs = np.abs(data_1['acs'] - data_1['ms'])\n",
    "abs_errsn = np.abs(data_1['acs'] - data_1['msn'])\n",
    "rel_errs = np.abs(data_1['acs'] - data_1['ms']) / np.abs(data_1['acs'])\n",
    "rel_errsn = np.abs(data_1['acs'] - data_1['msn']) / np.abs(data_1['acs'])\n",
    "\n",
    "dr = data_1['dr']\n",
    "gates = data_1['gates']\n",
    "\n",
    "crossings_data = data_1['crossings']\n",
    "depths = np.array([calculate_depth(qubits[idx] - 1, [(abs(c) - 1, c/abs(c)) for c in crossings_data[idx]]) for idx in tqdm.trange(len(qubits))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"processed/sims_1_props.npz\",\n",
    "    crossings=crossings, qubits=qubits, depths=depths,\n",
    "    abs_errs=abs_errs, abs_errsn=abs_errsn, rel_errs=rel_errs, rel_errsn=rel_errsn,\n",
    "    gates=gates, dr=dr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emulator Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = []\n",
    "for f in glob.glob(f'device_run/results/h1-1e/normal/*.json'):\n",
    "    with open(f, 'r') as file:\n",
    "        dict_results = json.load(file)\n",
    "        from pytket.backends.backendresult import BackendResult\n",
    "        res = BackendResult.from_dict(dict_results)\n",
    "        shots.append(res.get_shots())\n",
    "shots = np.concatenate(shots, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_conj = []\n",
    "for f in glob.glob(f'device_run/results/h1-1e/conj/*.json'):\n",
    "    with open(f, 'r') as file:\n",
    "        dict_results = json.load(file)\n",
    "        from pytket.backends.backendresult import BackendResult\n",
    "        res = BackendResult.from_dict(dict_results)\n",
    "        shots_conj.append(res.get_shots())\n",
    "shots_conj = np.concatenate(shots_conj, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits, data = shots[:, :15], shots[:, 15+16:] \n",
    "bits_conj, data_conj = shots_conj[:, :15], shots_conj[:, 15+16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(replicas: np.ndarray, alpha: float):\n",
    "    n = replicas.shape[0]\n",
    "    if n == 0:\n",
    "        return (0.0, 0.0)\n",
    "    elif n == 1:\n",
    "        return (replicas[0], replicas[0], 0.0)\n",
    "    alpha2 = scipy.stats.norm.cdf(np.sqrt(n / (n - 1)) * scipy.stats.t.ppf(alpha / 2, n - 1))\n",
    "    return tuple(np.quantile(replicas, [alpha2, 1.0 - alpha2])) + (np.std(replicas),)\n",
    "\n",
    "def do_replica(arg):\n",
    "    i, bits_orig, data_orig, braid, mitigation = arg\n",
    "    if i == 0:\n",
    "        data, bits = data_orig, bits_orig\n",
    "    else:\n",
    "        indices = np.random.randint(bits_orig.shape[0], size=bits_orig.shape[0])\n",
    "        data, bits = data_orig[indices, :], bits_orig[indices, :]\n",
    "        \n",
    "    mask = (np.all(data[:, 2:] == 0, axis=1) & (data[:, 0] == 0)).astype(float)\n",
    "    sign = 1 - 2 * data[:, 1].astype(float)\n",
    "    value = mask * sign\n",
    "\n",
    "    if mitigation:\n",
    "        xbits = data[:, 2:] ^ bits[:, 1:]\n",
    "        reject = (data[:, 0] != 0) | np.any((xbits[:, :-1] | xbits[:, 1:]) == 0, axis=1)\n",
    "        reals = value[(bits[:, 0] == 0) & ~reject]\n",
    "        imags = value[(bits[:, 0] == 1) & ~reject]\n",
    "        dr = np.mean(reject.astype(float), axis=0)\n",
    "    else:\n",
    "        reals = value[bits[:, 0] == 0]\n",
    "        imags = value[bits[:, 0] == 1]\n",
    "        dr = 0.0\n",
    "\n",
    "    est_raw = np.mean(reals, axis=0) + 1j * np.mean(imags, axis=0)\n",
    "    est = (-np.exp(-1j*3*np.pi/5))**(3*braid.writhe) * phi ** (data.shape[-1] - 2) * est_raw\n",
    "    \n",
    "    return i, est, est_raw, dr\n",
    "\n",
    "def compute_jones(bits: np.ndarray, data: np.ndarray, braid: Braid, mitigation: bool, bootstrap: bool, resamples: int = 100, alpha: float = 0.05):\n",
    "    if bootstrap:\n",
    "        pbar = tqdm.trange(resamples + 1, desc=\"processing replicas\")\n",
    "    else:\n",
    "        pbar = range(1)\n",
    "        resamples = 0\n",
    "\n",
    "    ests = [None]*(resamples + 1)\n",
    "    ests_raw = [None]*(resamples + 1)\n",
    "    drs = [None]*(resamples + 1)\n",
    "    for i, est, est_raw, dr in map(do_replica, ((i, bits, data, braid, mitigation) for i in pbar)):\n",
    "        ests[i] = est\n",
    "        ests_raw[i] = est_raw\n",
    "        drs[i] = dr\n",
    "    est = np.array(ests)\n",
    "    est_raw = np.array(ests_raw)\n",
    "    dr = np.array(drs)\n",
    "\n",
    "    return est[0], bootstrap_ci(est.real, alpha), bootstrap_ci(est.imag, alpha), dr[0], bootstrap_ci(dr, alpha), est_raw[0], bootstrap_ci(est_raw.real, alpha), bootstrap_ci(est_raw.imag, alpha), ests[1:], ests_raw[1:], drs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing replicas: 100%|██████████| 10001/10001 [00:03<00:00, 3068.41it/s]\n",
      "processing replicas: 100%|██████████| 10001/10001 [00:03<00:00, 3137.12it/s]\n",
      "processing replicas: 100%|██████████| 10001/10001 [00:02<00:00, 4458.17it/s]\n",
      "processing replicas: 100%|██████████| 10001/10001 [00:03<00:00, 3113.10it/s]\n",
      "processing replicas: 100%|██████████| 10001/10001 [00:02<00:00, 4531.19it/s]\n"
     ]
    }
   ],
   "source": [
    "res = compute_jones(bits, data, Braid.from_word(json.load(open(\"device_run/b_n15_l15.json\"))['original']['word']), True, True, 10000)\n",
    "resn = compute_jones(bits, data, Braid.from_word(json.load(open(\"device_run/b_n15_l15.json\"))['original']['word']), False, True, 10000)\n",
    "res_conj = compute_jones(bits_conj, data_conj, Braid.from_word(json.load(open(\"device_run/b_n15_l15.json\"))['original']['word']), True, True, 10000)\n",
    "resn_conj = compute_jones(bits_conj, data_conj, Braid.from_word(json.load(open(\"device_run/b_n15_l15.json\"))['original']['word']), False, True, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.13274241625389036-0.817078298378928j)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bspec = json.load(open(\"device_run/b_n15_l15.json\"))\n",
    "braid = Braid.from_word(bspec['original']['word'])\n",
    "actual_raw = (bspec['jones'][0] + 1j*bspec['jones'][1]) / ((-np.exp(-1j*3*np.pi/5))**(3*(braid.writhe)) * phi ** (data.shape[-1] - 2))\n",
    "actual_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conj_mitigate(z, zconj, ref):\n",
    "    z1 = abs(z) * np.sqrt(z / zconj)\n",
    "    r = abs(z + zconj) / 2\n",
    "    if z1.real < 0.0:\n",
    "        r *= -1.0\n",
    "    i = abs(z - zconj) / 2\n",
    "    if z1.imag < 0.0:\n",
    "        i *= -1.0\n",
    "    z2 = r + 1j * i\n",
    "    if abs(z2 - ref) > abs(-z2 - ref):\n",
    "        z2 *= -1.0\n",
    "    return z2\n",
    "\n",
    "val_conj_mitigated = conj_mitigate(res[5], res_conj[5], res[5]) \n",
    "val_conj_mitigated_replicas = np.array([conj_mitigate(z, zconj, val_conj_mitigated) for z, zconj in zip(res[-2], res_conj[-2])])\n",
    "\n",
    "val_conj_unmitigated = conj_mitigate(resn[5], resn_conj[5], res[5]) \n",
    "val_conj_unmitigated_replicas = np.array([conj_mitigate(z, zconj, val_conj_unmitigated) for z, zconj in zip(resn[-2], resn_conj[-2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"processed/emulator.npz\",\n",
    "    z_mit=res[5], z_mit_reps=res[-2],\n",
    "    z_unmit=resn[5], z_unmit_reps=resn[-2],\n",
    "    zc_mit=res_conj[5], zc_mit_reps=res_conj[-2],\n",
    "    zc_unmit=resn_conj[5], zc_unmit_reps=resn_conj[-2],\n",
    "    conj_mit=val_conj_mitigated, conj_mit_reps=val_conj_mitigated_replicas,\n",
    "    conj_unmit=val_conj_unmitigated, conj_unmit_reps=val_conj_unmitigated_replicas,\n",
    "    actual=actual_raw, writhe=braid.writhe, strands=braid.strands\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
